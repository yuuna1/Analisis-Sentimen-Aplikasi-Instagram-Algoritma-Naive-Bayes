# -*- coding: utf-8 -*-
"""Analisis-Sentimen-Aplikasi-Instagram.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ss5zUaSiODL4bwOx9DfK_t_wWmU4i4Sh

# **Analisis Sentimen Aplikasi Instagram Menggunakan Metode Naive Bayes**

### **Step 1: Mengumpulkan (Scraping) Data**
"""

#install library google-play-scraper
!pip install google-play-scraper

from google_play_scraper import app
import pandas as pd
import numpy as np

#scrape jumlah ulasan yang diinginkan
from google_play_scraper import Sort, reviews
result, continuation_token = reviews(
    'com.instagram.android', #Link token aplikasi yang ingin diambil ulasannya
    lang='id',
    country='id', #Set bahasa nya menjadi bahasa indonesia
    sort=Sort.MOST_RELEVANT, #Untuk menambil ulasan yang relevan
    count=1000, #Mengatur jumlah data yang ingin diambil
    filter_score_with=None # None untuk mengambil semua score atau rating bintang 1 sampai 5
)

#Untuk memastikan jumlah data yg didapatkan
len(result)

result = pd.DataFrame(np.array(result),columns=['review'])
result = result.join(pd.DataFrame(result.pop('review').tolist()))
result.head(10)

result.to_csv("Scrapped_instagram.csv", index = False)

#menimport data set yang ada
dataset = pd.read_csv("Scrapped_instagram.csv")

dataset.head(5)

#menfilter kolom data hasil scraping
dataset[['userName', 'score','at', 'content']].head()

#mengurutkan ulasan berdasarkan tanggal pembuatan

dataset = dataset[['userName', 'score','at', 'content']]
sorted_df = dataset.sort_values(by='at', ascending=False) #diurutkan berdasarkan Terbaru, jika ingin mengurutkan dari yang terlama true kan
sorted_df.head()

#kemudian hasil filter save menjadi file csv
sorted_df.to_csv("Sort_Instagram.csv", index = False)

#kemudian kita simpan ke variabel scrap_data
scrap_data = sorted_df[['userName', 'score','at', 'content']]

#karena hanya membutuhkan kolom content dan score maka dilakukan filter.
scrap_data= scrap_data[['content', 'score']]

scrap_data.head()

"""### **Step 2: Tahap Pelabelan**"""

#Tahap Pelabelan
def pelabelan(score):
  if score < 3: # ulasan skor dibawa 3 negatif
    return 'Negatif'
  elif score == 4 : # ulasan skor 4 positif
    return 'Positif'
  elif score == 5 : # ulasan skor 5 positif
    return 'Positif'
scrap_data['Label'] = scrap_data ['score'].apply(pelabelan)
scrap_data.head(10)

scrap_data.to_csv("LabelinInstagram.csv", index = False)

"""### **Step 3: Tahap Preprocessing**

**CLEANING**
"""

#Import data yang sudah diberi label

import pandas as pd
pd.set_option('display.max_columns', None)
scrap_data = pd.read_csv("LabelinInstagram.csv")
scrap_data.head(15)

# info() digunakan untuk menampilkan informasi detail tentang dataframe,
#seperti jumlah baris data, nama-nama kolom berserta jumlah data dan tipe datanya, dan sebagainya.
scrap_data.info()

#Tampilkan setiap baris yang memiliki nilai null (NaN) pada kolom apapun
#Gunakan fitur isna() yang disediakan library pandas
scrap_data.isna()

scrap_data.isna().any()

#mencari jumlah baris data yang bernilai null
#terdapat kolom label memiliki nilai kosong
scrap_data.isnull().sum()

scrap_data.dropna(subset=['Label'],inplace = True)

scrap_data.isnull().sum()

scrap_data.head(15)

scrap_data.to_csv("Cleaning_none.csv", index = False)

#import data yang sudah bersih dari ulasan netral
import pandas as pd
df = pd.read_csv('Cleaning_none.csv')
df.head(15)

"""**CLEAN TANDA BACA , NOMOR DLL , DAN CASE FOLDING**"""

import re
def  clean_text(df, text_field, new_text_field_name):
    scrap_data[new_text_field_name] = scrap_data[text_field].str.lower() #Mengubah menjadi huruf kecil semua (casefolding)
    #Menhapus tanda2 baca dan emoticon yang tidak dibutuhkan
    scrap_data[new_text_field_name] = scrap_data[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))
    #Menghapus nomor
    scrap_data[new_text_field_name] = scrap_data[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))
    return scrap_data

scrap_data['text_clean'] = scrap_data['content'].str.lower()
scrap_data['text_clean']
data_clean = clean_text(scrap_data, 'content', 'text_clean')
data_clean.head(10)

scrap_data.to_csv("Casefolding_Instagram.csv", index = False)

"""**STOPWORD REMOVAL**"""

#Install modul nltk
!pip install nltk

#Import library nltk corpus
import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('indonesian')
data_clean['text_StopWord'] = data_clean['text_clean'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))
data_clean.head(10)

"""**TOKENIZE**"""

import nltk
nltk.download('punkt') #memanggil fungsi punkt
from nltk.tokenize import sent_tokenize, word_tokenize
data_clean['text_tokens'] = data_clean['text_StopWord'].apply(lambda x: word_tokenize(x))
data_clean.head()

"""**STEMMING**"""

#Install library sastrawi
!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

#-----------------STEMMING -----------------
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# buat stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# proses stemming
def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}
hitung=0

for document in data_clean['text_tokens']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '

print(len(term_dict))
print("------------------------")
for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    hitung+=1
    print(hitung,":",term,":" ,term_dict[term])

print(term_dict)
print("------------------------")

def get_stemmed_term(document):
    return [term_dict[term] for term in document]


#memisahkan file eksekusinya setelah pembacaaan term selesai
data_clean['text_steamindo'] = data_clean['text_tokens'].apply(lambda x:' '.join(get_stemmed_term(x)))
data_clean.head(20)

data_clean.to_csv('hasil_TextPreProcessing_instagram.csv', index= False)

"""### **Step 4: Tahap Splitting Data**"""

#membagi data menjadi data training dan testing dengan test_size = 0.20 dan random state nya 0
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_clean['content'], data_clean['Label'],
                                                    test_size = 0.20,
                                                    random_state = 0)

"""### **Step 5: Tahap Pembobotan TF-IDF**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
tfidf_train = tfidf_vectorizer.fit_transform(X_train)
tfidf_test = tfidf_vectorizer.transform(X_test)

#Memastikan Split Data
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(X_train)

X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train)

"""### **Step 5: Tahap Metode Naive Bayes**"""

from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(tfidf_train, y_train)

X_train.toarray()

y_pred = nb.predict(tfidf_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

clf = MultinomialNB()
clf.fit(X_train, y_train)
predicted = clf.predict(X_test)

print("MultinomialNB Accuracy:", accuracy_score(y_test,predicted))
print("MultinomialNB Precision:", precision_score(y_test,predicted, average="binary", pos_label="Negatif"))
print("MultinomialNB Recall:", recall_score(y_test,predicted, average="binary", pos_label="Negatif"))
print("MultinomialNB f1_score:", f1_score(y_test,predicted, average="binary", pos_label="Negatif"))

print(f'confusion_matrix:\n {confusion_matrix(y_test, predicted)}')
print('====================================================\n')
print(classification_report(y_test, predicted, zero_division=0))

# Load dataset
data_clean = pd.read_csv('hasil_TextPreProcessing_instagram.csv')